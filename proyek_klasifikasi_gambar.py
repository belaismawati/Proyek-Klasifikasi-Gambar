# -*- coding: utf-8 -*-
"""Proyek Klasifikasi Gambar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z28gYaWCwUjEN47h-OdOnC6uCTlYijQp

# Proyek Klasifikasi Gambar: [Food Classification dataset](http://www.kaggle.com/datasets/harishkumardatalab/food-image-classification-dataset)
- **Nama:** Bela Ismawati Nuraisa
- **Email:** belaismawati292@gmail.com
- **ID Dicoding:** @belaismawati

## Import Semua Packages/Library yang Digunakan
"""

import os
import random
import shutil
import numpy as np
import pandas as pd
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.models import load_model
from IPython.display import FileLink

"""## Data Preparation

### Data Loading
"""

from google.colab import drive
drive.mount('/content/drive')

dataset_dir = '/content/drive/My Drive/Proyek Klasifikasi Gambar/Food Classification dataset'
# dataset_dir = '/kaggle/input/food-classification-dataset/Food Classification dataset'
print("Jumlah file dalam dataset:", len(os.listdir(dataset_dir)))

def print_images_resolution(directory):
    unique_sizes = set()
    total_images = 0

    for subdir in os.listdir(directory):
        subdir_path = os.path.join(directory, subdir)
        image_files = os.listdir(subdir_path)
        num_images = len(image_files)
        print(f"{subdir}: {num_images}")
        total_images += num_images

        for img_file in image_files:
            img_path = os.path.join(subdir_path, img_file)
            with Image.open(img_path) as img:
                unique_sizes.add(img.size)

        for size in unique_sizes:
            print(f"- {size}")
        print("---------------")

    print(f"\nTotal: {total_images}")

print_images_resolution(dataset_dir)

"""### Data Preprocessing

#### Split Dataset

- Membagi dataset dan menyimpannya di folder tiap subset
"""

split_dir = '/content/drive/My Drive/Proyek Klasifikasi Gambar'
# split_dir = '/kaggle/working/'
image_dict = defaultdict(list)

for class_name in sorted(os.listdir(dataset_dir)):
    class_path = os.path.join(dataset_dir, class_name)
    if not os.path.isdir(class_path):
      continue

    images = sorted([filename for filename in os.listdir(class_path) if filename.lower().endswith(('.jpg', '.jpeg', '.png'))])
    image_dict[class_name].extend(images)

# Mengonversi ke bentuk list
images, labels = [], []
for class_name, image_list in image_dict.items():
  for image in image_list:
    images.append(image)
    labels.append(class_name)

X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# Buat folder train, val, test berdasarkan label folder asli
for subset in ["train", "val", "test"]:
    for label in image_dict.keys():
        os.makedirs(os.path.join(split_dir, subset, label), exist_ok=True)

# Fungsi pemindahan file berdasarkan kategori folder asli
def move_files(image_list, label_list, subset_name):
    for image, label in zip(image_list, label_list):
        source_path = os.path.join(dataset_dir, label, image)
        destination_path = os.path.join(split_dir, subset_name, label, image)
        shutil.copy(source_path, destination_path)

# Pindahkan ke folder berdasarkan stratified split
move_files(X_train, y_train, "train")
move_files(X_val, y_val, "val")
move_files(X_test, y_test, "test")

print(f"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}")

"""- Load data train, val, dan test yang telah dibagi"""

train_dir = '/content/drive/My Drive/Proyek Klasifikasi Gambar/train'
val_dir = '/content/drive/My Drive/Proyek Klasifikasi Gambar/val'
test_dir = '/content/drive/My Drive/Proyek Klasifikasi Gambar/test'

image_dict = defaultdict(list)

for class_name in sorted(os.listdir(dataset_dir)):
    class_path = os.path.join(dataset_dir, class_name)
    if not os.path.isdir(class_path):
      continue

    images = sorted([filename for filename in os.listdir(class_path) if filename.lower().endswith(('.jpg', '.jpeg', '.png'))])
    image_dict[class_name].extend(images)

# Mengonversi ke bentuk list
images, labels = [], []
for class_name, image_list in image_dict.items():
  for image in image_list:
    images.append(image)
    labels.append(class_name)

class_names = sorted(image_dict.keys())
print(class_names)

plt.figure(figsize=(15, 10))

for i, class_name in enumerate(class_names):
    class_train_path = os.path.join(train_dir, class_name)

    if os.path.isdir(class_train_path) and os.listdir(class_train_path):
        first_image = sorted(os.listdir(class_train_path))[0]
        image_path = os.path.join(class_train_path, first_image)
        image = plt.imread(image_path)
        plt.subplot(4, 4, i + 1)
        plt.imshow(image)
        plt.title(class_name)
        plt.axis('off')

plt.tight_layout()
plt.show()

# Fungsi untuk mengubah format gambar ke RGB
def convert_to_rgb(folder_dir):
    images = []
    for class_name in os.listdir(folder_dir):
        class_path = os.path.join(folder_dir, class_name)

        if os.path.isdir(class_path):
            for img_name in os.listdir(class_path):
                img_path = os.path.join(class_path, img_name)

                try:
                    img = Image.open(img_path)
                    if img.mode != "RGB":
                        img = img.convert("RGB")
                        img.save(img_path)
                        print(f"{img_name} dikonversi ke RGB")
                except Exception as e:
                    print(f"Error pada {img_name}: {e}")

convert_to_rgb(train_dir)
convert_to_rgb(val_dir)
convert_to_rgb(test_dir)

train_gen = ImageDataGenerator(rescale=1./255,
                               rotation_range=20,
                               width_shift_range=0.2,
                               height_shift_range=0.2,
                               shear_range=0.2,
                               zoom_range=0.2,
                               horizontal_flip=True,
                               brightness_range=[0.5, 1.5])

val_gen = ImageDataGenerator(rescale=1./255)

test_gen = ImageDataGenerator(rescale=1./255)

train_generator = train_gen.flow_from_directory(train_dir,
                                                target_size=(224, 224),
                                                batch_size=32,
                                                class_mode="categorical",
                                                shuffle=True)

val_generator = val_gen.flow_from_directory(val_dir,
                                            target_size=(224, 224),
                                            batch_size=32,
                                            class_mode="categorical",
                                            shuffle=False)

test_generator = test_gen.flow_from_directory(test_dir,
                                             target_size=(224, 224),
                                             batch_size=32,
                                             class_mode="categorical",
                                             shuffle=False)

images, labels = next(iter(train_generator)) # Mengambil satu batch (default batch_size=32)
class_name_dict = train_generator.class_indices
inverse_class_name_dict = {value:key for key,value in class_name_dict.items()}

# Menyimpan index dari setiap kelas
shown_classes = set()
selected_indices = []

for i in range(len(labels)):
  label = np.argmax(labels[i])

  if label not in shown_classes:
    selected_indices.append(i)
    shown_classes.add(label)
  if len(shown_classes) == len(inverse_class_name_dict):
    break

# Tampilkan beberapa gambar dalam batch
plt.figure(figsize=(15, 10))
for i, image in enumerate(selected_indices):
    plt.subplot(2, 4, i+1)
    plt.imshow(images[image])
    label_image = np.argmax(labels[image])
    label_name = inverse_class_name_dict[label_image]
    plt.axis("off")
    plt.title(f"Kelas: {label_name}")

plt.tight_layout()
plt.show()

"""## Modelling"""

# Mengambil Feauture dari model pre-trained
pre_train_model = MobileNetV2(weights='imagenet',
                              include_top=False,
                              input_shape=(224,224,3))

for layer in pre_train_model.layers:
    layer.trainable = False

# Membuat Arsitektur Model
model = tf.keras.models.Sequential([
        pre_train_model,
        tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),
        tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),
        tf.keras.layers.MaxPool2D((2,2)),

        # tf.keras.layers.DepthwiseConv2D((3,3), padding='same'),
        tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),
        tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),
        tf.keras.layers.MaxPool2D((2,2)),

        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.7),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dense(7, activation='softmax')])

# Melakukan kompilasi Model
model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy'])

model.summary()

def callback():
    callbacks = []

    # ModelCheckpoint
    checkpoint_callback = ModelCheckpoint(filepath='best_model.h5',
                                save_best_only=True,
                                monitor='val_loss',
                                mode='min',
                                verbose=1)

    callbacks.append(checkpoint_callback)

    # ReduceLROnPlateau callback
    reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss',
                                           factor=0.3,
                                           patience=7,
                                           verbose=1,
                                           mode='auto',
                                           epsilon=0.0001,
                                           cooldown=1,
                                           min_lr=0.000001)

    callbacks.append(reduce_lr_callback)

    # Early stopping callback
    early_stopping = EarlyStopping(monitor='val_accuracy',
                                   patience=10,
                                   min_delta=0.001,
                                   mode='max',
                                   restore_best_weights=True,
                                   verbose=1)

    callbacks.append(early_stopping)

    return callbacks

callbacks = callback()

# Latih Modelnya
history = model.fit(train_generator,
                    epochs=50,
                    steps_per_epoch=len(train_generator),
                    validation_data=val_generator,
                    validation_steps=len(val_generator),
                    callbacks=[callbacks])

"""## Evaluasi dan Visualisasi"""

# Mengevalusasi Model dengan Data Uji
loss, acc = model.evaluate(test_generator)
print(f'Hasil Akurasi model adalah {acc:.3f} dengan loss {loss:.3f} pada data uji')

def plot_evaluation_model(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  acc = history.history['accuracy']
  val_acc = history.history['val_accuracy']

  epoch = len(loss)

  plt.figure()
  plt.plot(range(epoch), loss, label='Training Loss')
  plt.plot(range(epoch), val_loss, label='Validation Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()

  plt.figure()
  plt.plot(range(epoch), acc, label='Training Accuracy')
  plt.plot(range(epoch), val_acc, label='Validation Accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()

# Visualisasi Akurasi dan Loss
plot_evaluation_model(history)

class_name_dict = test_generator.class_indices
inverse_class_name_dict = {value: key for key, value in class_name_dict.items()}

# Melakukan prediksi
predictions = model.predict(test_generator)
predicted_classes = [np.argmax(predict) for predict in predictions]
predicted_labels = [inverse_class_name_dict[label] for label in predicted_classes]

actual_class_name = [inverse_class_name_dict[idx] for idx in test_generator.classes]

# Menyimpan hasil prediksi ke dalam CSV
result = pd.DataFrame({'Actual label': actual_class_name, 'Predicted label': predicted_labels})
result.to_csv('hasil_prediksi.csv', index=False)

# Menampilkan beberapa hasil prediksi
for i in range(10):
    print(f'Actual: {actual_class_name[i]}, Predicted: {predicted_labels[i]}')

classif_report = classification_report(actual_class_name, predicted_labels)

conf_matrix = confusion_matrix(actual_class_name, predicted_labels)

print('Classification Report:\n', classif_report)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True,
            fmt='d', cmap='Blues',
            xticklabels=class_name_dict,
            yticklabels=class_name_dict)
plt.xlabel('Predicted Label')
plt.ylabel('Actual Label')
plt.title('Confusion Matrix')
plt.show()

"""## Konversi Model"""

# Menyimpan model dalam format .h5
model.save('model_food_classification.h5')

"""* **Konversi ke SavedModel**"""

# Load model dari format .h5
model = load_model('model_food_classification.h5')

# Mengubah format model ke SavedModel
tf.saved_model.save(model, 'my_model/')

print(os.listdir('my_model/'))

# Kompres folder my_model/ menjadi .zip
shutil.make_archive("my_model", 'zip', "my_model")

# Menampilkan link untuk mengunduh file zip-nya
FileLink("my_model.zip")

"""* **Konversi model ke format TF-Lite**"""

model = tf.saved_model.load("my_model")
print(model.signatures)

# Konversi ke format TFLite
converter = tf.lite.TFLiteConverter.from_saved_model("my_model")
tflite_model = converter.convert()

# Simpan ke file
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

# Buat konten yang akan ditulis ke dalam file
content = """Baked Potato
Crispy Chicken
Donut
Fries
Hot Dog
Sandwich
Taco
Taquito"""

# Tentukan path dan nama file
file_path = "klasifikasiGambar.txt"

# Tulis konten ke dalam file
with open(file_path, "w") as file:
    file.write(content)

# Menampilkan link untuk mengunduh model
FileLink('model.tflite')

"""* **Konversi model ke TFJS**"""

!pip install tensorflowjs

# Konversi dari direktori SavedModel
!tensorflowjs_converter --input_format=tf_saved_model my_model/ tfjs_model/

print("Detected GPUs:", tf.config.list_physical_devices('GPU'))

# Kompres folder
shutil.make_archive("tfjs_model", 'zip', "tfjs_model")

# Menampilkan link untuk mengunduh file zip-nya
FileLink("tfjs_model.zip")

"""## Inference (Optional)"""

import os
import numpy as np
from PIL import Image
import tensorflow as tf

# Load SavedModel
model_dir = 'my_model/'
loaded_model = tf.saved_model.load(model_dir)
inference_fn = loaded_model.signatures['serving_default']
# print(loaded_model.signatures.keys())

target_size = (224, 224)
image_path = "Crispy Chicken.jpg"

def preprocessing_image(image_path):
    img = Image.open(image_path)
    img = img.convert('RGB').resize(target_size)
    img = np.array(img).astype(np.float32) / 255.0
    img = np.expand_dims(img, axis=0)

    return img

# Memproses gambar
image = preprocessing_image(image_path)

# Konversi ke tensor dan melakukan prediksi
batch_tensor = tf.convert_to_tensor(image)
result = inference_fn(batch_tensor)
# print(result.keys())

predictions = list(result.values())[0]
# predictions = model.predict(batch_tensor)

# Map labels sesuai urutan class
map_labels = {0: 'Baked Potato',
              1: 'Crispy Chicken',
              2: 'Donut',
              3: 'Fries',
              4: 'Hot Dog',
              5: 'Sandwich',
              6: 'Taco',
              7: 'Taquito'}

# Menampilkan hasil prediksi
for i, prediction in enumerate(predictions):
    predicted_class = tf.argmax(prediction).numpy()
    predicted_label = map_labels[predicted_class]
    print(f"Prediksi: {predicted_label}")

tf.keras.backend.clear_session()

